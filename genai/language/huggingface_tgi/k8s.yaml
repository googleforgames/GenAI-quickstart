# Huggingface TGI Deployments
#
# We create two different deployments that each match the TGI service at the bottom. If you want to play with a different
# configuration (CPU vs 2xL4 x 4xL4), scale the replicas up and down.
#
apiVersion: apps/v1
kind: Deployment
metadata:
  name: huggingface-tgi-mixtral-big
  labels:
    name: huggingface-tgi-mixtral-big
spec:
  replicas: 0
  selector:
    matchLabels:
      name: huggingface-tgi-api
      tgi-config: mixtral-8x7B-8xL4
  template:
    metadata:
      labels:
        name: huggingface-tgi-api
        tgi-config: mixtral-8x7B-8xL4
    spec:
      serviceAccountName: k8s-sa-aiplatform
      nodeSelector:
        cloud.google.com/gke-accelerator: "nvidia-l4"
        cloud.google.com/gke-ephemeral-storage-local-ssd: "true"
        cloud.google.com/compute-class: "Accelerator"
      containers:
        - name: huggingface-tgi-api
          ports:
            - containerPort: 80
          image: ghcr.io/huggingface/text-generation-inference:1.4.2
          # Use this image for Gemma support:
          # image: us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-hf-tgi-serve:20240220_0936_RC01
          args:
            # Choose models with multiturn chat support
            # Look for `chat_template`, e.g.: https://huggingface.co/HuggingFaceH4/zephyr-7b-beta/blob/main/tokenizer_config.json#L34
            # Alternatives if you don't have an API Key:
            # - --model-id=mistralai/Mistral-7B-Instruct-v0.2
            # - --model-id=HuggingFaceH4/zephyr-7b-beta
            #
            # To run Gemma:
            # - --model-id=google/gemma-7b-it
            # (but you'll need the updated image above)
            #
            # Another alternative, but you'll need a bunch of ephemeral storage:
            - --model-id=mistralai/Mixtral-8x7B-Instruct-v0.1
            # To run on L4s we have to quantize
            # - --quantize=bitsandbytes-nf4
            - --quantize=eetq
            #
            # --num-shard should match nvidia.com/gpu: limits
            - --num-shard=8
            #
            # raise the default input/total tokens to allow for more chat context
            - --max-input-length=3072
            - --max-total-tokens=4096
          # To use Gemma, you need to uncomment and fill this in. (Preferably, use a secret or a secret manager instead.)
          # env:
          # - name: HUGGING_FACE_HUB_TOKEN
          #   value: <your API key>
          resources:
            requests:
              cpu: "20"
              memory: "250Gi"
              ephemeral-storage: "100Gi"
              nvidia.com/gpu: 8
            limits:
              cpu: "90"
              memory: "320Gi"
              ephemeral-storage: "200Gi"
              nvidia.com/gpu: 8
          volumeMounts:
            - mountPath: /dev/shm
              name: shm
            - mountPath: /data
              name: data
      volumes:
        # # c.f. https://github.com/huggingface/text-generation-inference#a-note-on-shared-memory-shm
        - name: shm
          emptyDir:
            medium: Memory
            sizeLimit: 1Gi
        - name: data
          emptyDir: {}
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: huggingface-tgi-mixtral-small
  labels:
    name: huggingface-tgi-mixtral-small
spec:
  replicas: 0
  selector:
    matchLabels:
      name: huggingface-tgi-api
      tgi-config: mixtral-8x7B-2xL4
  template:
    metadata:
      labels:
        name: huggingface-tgi-api
        tgi-config: mixtral-8x7B-2xL4
    spec:
      serviceAccountName: k8s-sa-aiplatform
      nodeSelector:
        cloud.google.com/gke-accelerator: "nvidia-l4"
        cloud.google.com/gke-ephemeral-storage-local-ssd: "true"
        cloud.google.com/compute-class: "Accelerator"
      containers:
        - name: huggingface-tgi-api
          ports:
            - containerPort: 80
          image: ghcr.io/huggingface/text-generation-inference:1.4.2
          # Use this image for Gemma support:
          # image: us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-hf-tgi-serve:20240220_0936_RC01
          args:
            # Choose models with multiturn chat support
            # Look for `chat_template`, e.g.: https://huggingface.co/HuggingFaceH4/zephyr-7b-beta/blob/main/tokenizer_config.json#L34
            # Alternatives if you don't have an API Key:
            # - --model-id=mistralai/Mistral-7B-Instruct-v0.2
            # - --model-id=HuggingFaceH4/zephyr-7b-beta
            #
            # To run Gemma:
            # - --model-id=google/gemma-7b-it
            # (but you'll need the updated image above)
            #
            # Another alternative, but you'll need a bunch of ephemeral storage:
            - --model-id=mistralai/Mixtral-8x7B-Instruct-v0.1
            # To run on 2xL4s we have to quantize even smaller
            - --quantize=bitsandbytes-nf4
            # - --quantize=eetq
            #
            # --num-shard should match nvidia.com/gpu: limits
            - --num-shard=2
            #
            # raise the default input/total tokens to allow for more chat context
            - --max-input-length=3072
            - --max-total-tokens=4096
          # To use Gemma, you need to uncomment and fill this in. (Preferably, use a secret or a secret manager instead.)
          # env:
          # - name: HUGGING_FACE_HUB_TOKEN
          #   value: <your API key>
          resources:
            requests:
              cpu: "10"
              memory: "80Gi"
              ephemeral-storage: "100Gi"
              nvidia.com/gpu: 2
            limits:
              cpu: "20"
              memory: "160Gi"
              ephemeral-storage: "200Gi"
              nvidia.com/gpu: 2
          volumeMounts:
            - mountPath: /dev/shm
              name: shm
            - mountPath: /data
              name: data
      volumes:
        # # c.f. https://github.com/huggingface/text-generation-inference#a-note-on-shared-memory-shm
        - name: shm
          emptyDir:
            medium: Memory
            sizeLimit: 1Gi
        - name: data
          emptyDir: {}
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: huggingface-tgi-mistral-cpu
  labels:
    name: huggingface-tgi-mistral-cpu
spec:
  replicas: 1
  selector:
    matchLabels:
      name: huggingface-tgi-api
      tgi-config: mistral-7B-cpu
  template:
    metadata:
      labels:
        name: huggingface-tgi-api
        tgi-config: mistral-7B-cpu
    spec:
      serviceAccountName: k8s-sa-aiplatform
      nodeSelector:
        cloud.google.com/gke-ephemeral-storage-local-ssd: "true"
        cloud.google.com/compute-class: "Performance"
        cloud.google.com/machine-family: "c3"
      containers:
        - name: huggingface-tgi-api
          ports:
            - containerPort: 80
          image: ghcr.io/huggingface/text-generation-inference:1.4.2
          # Use this image for Gemma support:
          # image: us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-hf-tgi-serve:20240220_0936_RC01
          args:
            # Choose models with multiturn chat support
            # Look for `chat_template`, e.g.: https://huggingface.co/HuggingFaceH4/zephyr-7b-beta/blob/main/tokenizer_config.json#L34
            # Alternatives if you don't have an API Key:
            - --model-id=mistralai/Mistral-7B-Instruct-v0.2
            # - --model-id=HuggingFaceH4/zephyr-7b-beta
            #
            # To run Gemma:
            # - --model-id=google/gemma-7b-it
            # (but you'll need the updated image above)
            #
            # Another alternative, but you'll need a bunch of ephemeral storage:
            # - --model-id=mistralai/Mixtral-8x7B-Instruct-v0.1
            # To run on L4s we have to quantize
            # - --quantize=bitsandbytes-nf4
            # - --quantize=eetq
            #
            # --num-shard should match nvidia.com/gpu: limits
            # - --num-shard=1
            #
            # raise the default input/total tokens to allow for more chat context
            - --max-input-length=3072
            - --max-total-tokens=4096
          # To use Gemma, you need to uncomment and fill this in. (Preferably, use a secret or a secret manager instead.)
          # env:
          # - name: HUGGING_FACE_HUB_TOKEN
          #   value: <your API key>
          resources:
            requests:
              cpu: "48"
              memory: "115Gi"
              ephemeral-storage: "50Gi"
            limits:
              cpu: "48"
              memory: "115Gi"
              ephemeral-storage: "50Gi"
          volumeMounts:
            - mountPath: /dev/shm
              name: shm
            - mountPath: /data
              name: data
      volumes:
        # # c.f. https://github.com/huggingface/text-generation-inference#a-note-on-shared-memory-shm
        - name: shm
          emptyDir:
            medium: Memory
            sizeLimit: 1Gi
        - name: data
          emptyDir: {}
---
apiVersion: v1
kind: Service
metadata:
  labels:
    name: huggingface-tgi-api
  name: huggingface-tgi-api
spec:
  ports:
  - name: http
    port: 8080
    targetPort: 80
    protocol: TCP
  selector:
    name: huggingface-tgi-api
  sessionAffinity: None
  type: ClusterIP
