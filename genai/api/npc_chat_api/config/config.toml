# Copyright 2024 Google LLC All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

[global]
database = "Spanner"

# GenAI provider - GKEGenAI or VertexAI. Note that switching GenAI implementations switches the
# embedding model requiring a data regeneration using the /reset_world_data endpoint.
genai = "GKEGenAI"
# genai = "VertexAI"


[Spanner]
instance_id = "npc-chat"
database_id = "npc-chat"

[GKEGenAI]
# embeddings_endpoint implements the API in the api/language/embeddings
embeddings_endpoint = "http://embeddings-api/"
embeddings_model = "sentence-transformers/multi-qa-MiniLM-L6-cos-v1"

# Uncomment ChatCompletions to use inference endpoint that implements the v1/chat/completions API: https://platform.openai.com/docs/api-reference/chat/create
# Chat Completions is implemented by the HuggingFace TGI >=1.4.0: https://huggingface.co/docs/text-generation-inference/en/messages_api,
completions = "ChatCompletions"

# Gemma is not quite supported so we manually generate using a template.
# completions = "ChatCompletionTemplate"

[GKEGenAI.ChatCompletions]
endpoint = "http://huggingface-tgi-api:8080/v1/"
model = "tgi" # irrelevant when using TGI
use_system_for_context = false # Does the model support a system prompt? Otherwise we have to send it as an initial chat.

[GKEGenAI.ChatCompletions.params]
temperature = 0.8
# stop = ['<|user|>'] # some models continue past a turn, this can help stop the model
max_tokens = 1024

[GKEGenAI.ChatCompletionTemplate]
endpoint = "http://huggingface-tgi-api:8080/generate"

# Use this for Gemma 7B:
#   Copied from https://huggingface.co/google/gemma-7b-it/blob/main/tokenizer_config.json
chat_template = "{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n' + message['content'] | trim + '<end_of_turn>\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n'}}{% endif %}"
# Does the chat template support using a "system" message for context?
use_system_for_context = false

[VertexAI]
embedding_model = "textembedding-gecko@003"
chat_model = "chat-bison"
